oops:
	What is OOP?
		Oop is a programming paradigm that deals with objects , which are a combination of data and code . The principles of oop are: Abstraction,Encapsulation,Inheritance,Polymorphism

	Abstraction:
		i) Abstraction's main goal is to handle complexity by hiding unnecessary details from the user.

		ii) Abstract Class : Keyword, Cannot creat object of abstract class , child class needs to implement if it is a pure virtual function. In cpp the class will be called as a virtual class if it has at least 1 pure virtual function. (virtual <return type> <function name>(params); , PURE: virtual <return type> <function name>(params) = 0; )

		iii) Real World Example: Car has engine, wheels wiring but user only needs to know driving to operate the car.

		iv) Code Example: function

	Encapsulation:
		i) Wrapping up of data members and member functions, this provides abstraction to the class.

		ii) Data hiding : (bulb behind a wall, person will only be able to see the wall) for coding , child function hides parent function's attributes if its the same . 

		iii) Data Shadowing: Local variables shadowing class variables .

		iv) Real Life Example: cell, human body, capsule

		v) Code Example : Class, Structure

	Inheritance: 
		i) Inheritance in OOP = When a class derives from another class. The child class will inherit all the public and protected properties and methods from the parent class. In addition, it can have its own properties and methods. 

		ii) this - refers to currently calling object, super - refers to immediate parent 

		iii) C++ Example : class a: <access specifier> b{<class>},<access specifier> c{<class>},...;
			 Python Example : class a(b,c,..):<class>

		iv) Types of inheritences : Single (a->b),multilevel(a->b->c),multiple(a,b->c),hirarchial(a->b,c) , hybrid (a->b,c->d)

		v) Real life example : Vehicles -> Cars, Buses, Bikes, Trucks


	Polymorphism:
		i) Polymorphism is the ability of an object to take on many forms. The most common use of polymorphism in OOP occurs when a parent class reference is used to refer to a child class object. 

		ii) Types of Polymorphism : Static(Overloading - function & operator) & Dynamic(Overriding)

		iii) Function Overloading VS Function Overriding : https://bit.ly/3pNChnI

		iv ) Operator Overloadign:
			1) Un-overloadable operators : . , :: , ?: , sizeof()
			2) Example: <returntype> operator <operatorsymbol>(<params>){<fn>}

		v) Real Life Example: Mother is wife for father or sister for uncle, 

		vi) Coding Examples: function overloading, method overriding (class inherited functions)


	Limitation:
	1. Not suited for small problems.
	2. Requires planning and design
	3. Intensive testing required

	OOPS vs Structural: (look up table on google)

	high level vs low level 

	what is static keyword


Data Structures: (https://www.geeksforgeeks.org/data-structures/)

	I) Linear Data Structures
		1) Arrays :  insertion/deletion/unsorted search O(N), sorted search O(log N), access time O(1). Ex: Storing all marks for students in a class.
		2) Linked lists, 3 types : singly,doubly,circular. Search/Access time O(N), insertion/deletion O(1) 
		3) Stack: Insertion/deletion/Access O(1), search O(N)
		4) Queue: Types: normal, double , circular, dynamic , Insertion/deletion/access O(1). Uses: Any situation where resources are shared among multiple users and served on first come first server basis. Examples include CPU scheduling, Disk Scheduling.
	II) Binary Tree, BST, Heap and Hash
		1) Binary Tree: 
			i) Traversals
				>Depth First Traversal: Inorder (Left-Root-Right), Preorder (Root-Left-Right) and Postorder (Left-Right-Root) 
				>Breadth First Traversal: Level Order Traversal 
			ii) Maximum number of nodes = 2^(h) – 1. IE h = 3 , 7 max nodes
			iii) Time Complexity of Tree Traversal: O(n)
			iv) Examples: One reason to use binary tree or tree in general is for the things that form a hierarchy. They are useful in File structures where each file is located in a particular directory and there is a specific hierarchy associated with files and directories. Another example where Trees are useful is storing hierarchical objects like JavaScript Document Object Model considers HTML page as a tree with nesting of tags as parent child relations. 

		2) Binary Search Tree
			i) Search :  O(h),Insertion : O(h),Deletion : O(h),Extra Space : O(n) for pointers,If Binary Search Tree is Height Balanced, then h = O(Log n) . Self-Balancing BSTs such as AVL Tree, Red-Black Tree and Splay Tree make sure that height of BST remains O(Log n)
			ii) Examples: Its main use is in search application where data is constantly entering/leaving and data needs to printed in sorted order. For example in implementation in E-commerce websites where a new product is added or product goes out of stock and all products are listed in sorted order. 

		3) Heaps
			i) Creation : 
				The root element will be at Arr[0].
				Arr[(i-1)/2]	Returns the parent node
				Arr[(2*i)+1]	Returns the left child node
				Arr[(2*i)+2]	Returns the right child node
			ii) Time Complexity:
				Get Minimum in Min Heap: O(1) [Or Get Max in Max Heap]
				Extract Minimum Min Heap: O(Log n) [Or Extract Max in Max Heap]
				Decrease Key in Min Heap: O(Log n)  [Or Decrease Key in Max Heap]
				Insert: O(Log n) 
				Delete: O(Log n)
			iii) Example : Used in implementing efficient priority-queues, which in turn are used for scheduling processes in operating systems. Priority Queues are also used in Dijikstra’s and Prim’s graph algorithms. 
		4) Hash
			i) Time Complexity:
				Space : O(n)
				Search    : O(1) [Average]    O(n) [Worst case]
				Insertion : O(1) [Average]    O(n) [Worst Case]
				Deletion  : O(1) [Average]    O(n) [Worst Case]
			ii) Example : Hashing can be used to remove duplicates from a set of elements. Can also be used find frequency of all items. For example, in web browsers, we can check visited usingrls using hashing. In firewalls, we can use hashing to detect spam. We need to hash IP addresses. Hashing can be used in any situation where want search() insert() and delete() in O(1) time. 

	III) Graph, Trie, Segment Tree and Suffix Tree
		1) Graph
			i) Time complexity : 
				Time Complexities in case of Adjacency Matrix :
				Traversal :(By BFS or DFS) O(V^2)
				Space : O(V^2)

				Time Complexities in case of Adjacency List :
				Traversal :(By BFS or DFS) O(V + E)
				Space : O(V+E)
		2) Trie
			i) Trie is an efficient data structure for searching words in dictionaries, search complexity with Trie is linear in terms of word (or key) length to be searched. If we store keys in binary search tree, a well balanced BST will need time proportional to M * log N, where M is maximum string length and N is number of keys in tree. Using trie, we can search the key in O(M) time. So it is much faster than BST.
			ii) Example : The most common use of Tries is to implement dictionaries due to prefix search capability. Tries are also well suited for implementing approximate matching algorithms, including those used in spell checking. It is also used for searching Contact from Mobile Contact list OR Phone Directory.
		3) Segment Tree
			i) This data structure is usually implemented when there are a lot of queries on a set of values. These queries involve minimum, maximum, sum, .. etc on a input range of given set. Queries also involve updation of values in given set. Segment Trees are implemented using array.
			ii) Time Complexities:
				Construction of segment tree : O(N)
				Query : O(log N)
				Update : O(log N)
				Space : O(N) [Exact space = 2*N-1]
		4) Suffix Tree
			i)	Suffix Tree is compressed trie of all suffixes, so following are very abstract steps to build a 	suffix tree from given text.
				> Generate all suffixes of given text.
				> Consider all suffixes as individual words and build a compressed trie.
			ii) Uses: When the same suffix tree will be used to check substrings/ substring length find longest substring etc.

Algorthims: 
	1) Searching and Sorting Algorithms:
		
		i) Binary Search : For a given sorted array , take middle element , check if its smaller or greater, go to right middle or left middle until search element = middle element , if lower index = upper index and search element != middle element then element does not exist . O(log N)

		ii) Bubble Sort	: Bubble Sort works by repeatedly swapping the adjacent elements if they are in wrong order. O(N^2)

		iii) Selection Sort : The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning. The algorithm maintains two subarrays in a given array.
			1) The subarray which is already sorted.
			2) Remaining subarray which is unsorted.
		In every iteration of selection sort, the minimum element (considering ascending order) from the unsorted subarray is picked and moved to the sorted subarray. O(N^2)

		iv) Insertion Sort:
		 	To sort an array of size n in ascending order:
			1: Iterate from arr[1] to arr[n] over the array.
			2: Compare the current element (key) to its predecessor.
			3: If the key element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element.

		#NOTE: Selection sort has a time complexity of O(N^2) always , whereas most of the time insertion sort has a time complexity of less than O(N^2). For smaller n , insertion sort is preferred over quicksort because of the overhead from recursive calls by quicksort.

		v) Quicksort :
			Like Merge Sort, QuickSort is a Divide and Conquer algorithm. It picks an element as pivot and partitions the given array around the picked pivot. The key process in quickSort is partition(). Target of partitions is, given an array and an element x of array as pivot, put x at its correct position in sorted array and put all smaller elements (smaller than x) before x, and put all greater elements (greater than x) after x. All this should be done in linear time. Theta(N log N), O(N^2).

		vi) Mergesort :
			Conceptually, a merge sort works as follows:
				Divide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted).
				Repeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list. O(N log N) , Space : O(N)

		# NOTE: Merge sort is more efficient and works faster than quick sort in case of larger array size or datasets. Quick sort is more efficient and works faster than merge sort in case of smaller array size or datasets. Sorting method : The quick sort is internal sorting method where the data is sorted in main memory. However O(N) auxilary space is required additionally.

		vii) Heapsort:
			Heap Sort Algorithm for sorting in increasing order: 
				1) Build a max heap from the input data. 
				2) At this point, the largest item is stored at the root of the heap. Replace it with the last item of the heap followed by reducing the size of heap by 1. Finally, heapify the root of the tree. Heapify Time Complexity: O(log N)
				3) Repeat step 2 while size of heap is greater than 1. O(N)
			O(N * log N)

		viii) Treesort:
			Create a binary search tree and do inorder traversal. O(N log N) , Space : O(N)

		ix) Shellsort: ShellSort is mainly a variation of Insertion Sort. In insertion sort, we move elements only one position ahead. When an element has to be moved far ahead, many movements are involved. The idea of shellSort is to allow exchange of far items. In shellSort, we make the array h-sorted for a large value of h. We keep reducing the value of h until it becomes 1. An array is said to be h-sorted if all sublists of every h’th element is sorted.

		x) Bucket Sort: O(N^2), average O(N+K), Space : O(N)
			1) Create n empty buckets (Or lists).
			2) Do following for every array element arr[i].
			.......a) Insert arr[i] into bucket[n*array[i]]
			3) Sort individual buckets using insertion sort.
			4) Concatenate all sorted buckets.

		xi) Counting Sort : O(N+K), Space : O(K)
		It works by counting the number of objects having distinct key values (kind of hashing). Then doing some arithmetic to calculate the position of each object in the output sequence.

		#NOTE: K is the range of inputs, Lets say: if 0 < a[i] < 100k and n < 10 even then , k = 100k.

		xii) Radix Sort: O(nk), Space: O(N+K)
		The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort.


		#NOTE: K for radix sort is = max number of digits

C++:

	1) Volatile : Volatile directs the compiler that the variable can be changed externally. Hence avoiding compiler optimization on the variable reference. [ const volatile int i = 10;]

	2) Inline Functions : The inline functions are faster in execution when compared to normal functions as the compiler treats inline functions as macros. [ inline int abc(params){fn}]

	3) A storage class defines the scope (visibility) and life-time of variables and/or functions within a C Program. They precede the type that they modify. We have four different storage classes in a C program −
		i) auto : Local
		ii) register : In register instead of ram
		iii) static : A static variable does exit though the objects for the respective class are not created. Static member variable share a common memory across all the objects created for the respective class. A static member variable can be referred using the class name itself.

		iv) extern: The extern storage class is used to give a reference of a global variable that is visible to ALL the program files.

	4) Static Functions : A static member function can be invoked using the class name as it exits before class objects comes into existence. It can access only static members of the class. (Static functions are useful when you don't want to create an instance of an object just to execute one public function on it.)

	5) Running c++ Program : 
		1) Compile ( .cpp -> .o)
		2) Link all of the libraries (.o -> exe/out)

	6) Wide Char: Useful for UNICODE , has much higher storage than a normal char. (256:char , 65536 :wide char ,      wchar_t wide;)

	7) Standard Streams in C++ : cin, cout, cerr and clog.

	8) Some functions that can be used in C and C++ :
		malloc(): allocates the requested memory and returns a pointer to it
		calloc(): Same as malloc except it sets allocated memory to zero
		free(): If any of the above 2 commmands are used we must use free instead of "delete"

	9) Copy Constructor:     ClassName (const ClassName &old_obj); 


	10) Friend Function : Friend Class A friend class can access private and protected members of other class in which it is declared as friend. It is sometimes useful to allow a particular class to access private members of other class. For example a LinkedList class may be allowed to access private members of Node.(Only in CPP)

	11) Struct vs class: struct cannot have functions, constructor-destructor cannot be parameter less in struct, struct can have access specifier, no inheritance in struct, default is public for structure but class private.

	12) The scope resolution operator is used to

		Resolve the scope of global variables.
		To associate function definition to a class if the function is defined outside the class.

	13) Namespace: A namespace is the logical division of the code which can be used to resolve the name conflict of the identifiers by placing them under different name space.

	14) We can compile a c++ without main fn but cannot execute it without main

	15) Tokens: A C++ program consists of various tokens and a token is either a keyword, an identifier, a constant, a string literal, or a symbol.

	16) Preprocessor is a directive to the compiler to perform certain things before the actual compilation process begins.

	17) Containers : A container is a holder object that stores a collection of other objects (its elements). They are implemented as class templates, which allows a great flexibility in the types supported as elements.	Containers replicate structures very commonly used in programming: dynamic arrays (vector), queues (queue), stacks (stack), heaps (priority_queue), linked lists (list), trees (set), associative arrays (map)...

	18) Templates: template <typename T> T myMax(T x, T y) 

	19) Ways to pass parameters :
		1) Call by value − We send only values to the function as parameters. We choose this if we do not want the actual parameters to be modified with formal parameters but just used.

		2) Call by address − We send address of the actual parameters instead of values. We choose this if we do want the actual parameters to be modified with formal parameters.

		3) Call by reference − The actual parameters are received with the C++ new reference variables as formal parameters. We choose this if we do want the actual parameters to be modified with formal parameters.

	20) delete[] : deletes memory allocated by new[]
	
	21) seekg(), seekp() : Used to move pointer for reading / writing data to a stream.

	22) Streams :
		> Ios Base -> istream(cin) , ostream(cout) -> (ifstream),(ofstream) 
		i) cout: cout is the object of ostream class. The stream ‘cout’ is by default connected to console output device.
		ii) cin: cin is the object of istream class. The stream ‘cin’ is by default connected to console input device.
		iii) ifstream/ofstream : Reading/Writing Files

	23) Braces and double quote difference : If a header file is included with in < > then the compiler searches for the particular header file only with in the built in include path. If a header file is included with in “ “, then the compiler searches for the particular header file first in the current working directory, if not found then in the built in include path

	24) Actual and Formal Parameters : The parameters sent to the function at calling end are called as actual parameters while at the receiving of the function definition called as formal parameters.

	25) Objects Destroyed in which order: The objects are destroyed in the reverse order of their creation.






Python:

	Python cons and pros

	pep : documentation of python
	Python Memory manager: 	- Private heap space
							- All obj stored in heap and private access to programmer
							- Inbuilt garbage collection

	Namespace : A namespace is a declarative region that provides a scope to the identifiers (the names of types, functions, variables, etc). Keeps dict of namespaces for variables: ranges from local, global, builtin. 

	Scope in python 

	What is scope resolution ? (local prefered over global scope if name is same for both types)

	What is decorator? (decorator: overlaps another function, it takes output of original function and adds something to the output , ie decorates the output)

	List Comprehension: [i for i in range(n)]

	Dict Comprehension : {i:0 for i in range(n)}

	Lambda in python: n Python, an anonymous function is a function that is defined without a name. While normal functions are defined using the def keyword in Python, anonymous functions are defined using the lambda keyword. Hence, anonymous functions are also called lambda functions.

	Shallow and Deep Copy : Refrence obj copied recursivily ,in deep and not in shallow

	range vs xrange : range returns list and xrange returns xrange objects

	pickle: - serializes object , unpickle : deserializes
			- Can be compressed
			- Portable

	generators : returns iterable collection one at time

	dir() : returns list of attributes and functions 

	.pyc:	Python compliles the .py files and save it as .pyc file
			The .pyc contain the compiled bytecode of Python source files, 
			A .pyc is not created for your main program file that you execute (only for imported modules).
			The .pyc file contains encoded python bytecode. (https://www.youtube.com/watch?v=0BhSWyDEDC4)

	*args and **quargs 

	Exception: BaseException

Software Engineering:

	Q.What is software process or Software Development Life Cycle (SDLC)?

	A.Software Development Life Cycle, or software process is the systematic development of software by following every stage in the development process namely, Requirement Gathering, System Analysis, Design, Coding, Testing, Maintenance and Documentation in that order.

	Q.What are SDLC models available?

	A. There are several SDLC models available such as :
		Waterfall Model (Illustrates Linear sequentail flow in SDLC), 
		Iterative Model(At each iteration, design modifications are made and new functional capabilities are added, IE: life cycle repeat), 
		Spiral model (provides support for risk management, it is spiral with many loops,each loop is a phase in software development. Radius represent cost and project so far , angular dimension represents progress in current phase), 
		V-model( also known as Verification and Validation model, each phase of SDLC is tested ),

	Cohesion and Coupling : Coupling and cohesion are terms which occur together very frequently. Coupling refers to the interdependencies between modules, while cohesion describes how related the functions within a single module are. High cohesion, low coupling best practice. In essence, high cohesion means keeping parts of a code base that are related to each other in a single place. Low Couplingg, at the same time, is about separating unrelated parts of the code base as much as possible.

	What is black-box and white-box testing? Black-box testing checks if the desired outputs are produced when valid input values are given. It does not verify the actual implementation of the program. White-box testing not only checks for desired and valid output when valid input is provided but also it checks if the code is implemented correctly.

	Quality assurance vs. Quality Control? Quality Assurance monitors to check if proper process is followed while software developing the software. Quality Control deals with maintaining the quality of software product.


DBMS:
	Why use normalization : - Reduce Redundancy
							- Reduce time for queries

	Types of languages in DBMS:
		DDL: DDL is Data Definition Language which is used to define the database and schema structure by using some set of SQL Queries like CREATE, ALTER, TRUNCATE, DROP and RENAME.
		DCL: DCL is Data Control Language which is used to control the access of the users inside the database by using some set of SQL Queries like GRANT and REVOKE.
		DML: DML is Data Manipulation Language which is used to do some manipulations in the database like Insertion, Deletion, etc. by using some set of SQL Queries like SELECT, INSERT, DELETE and UPDATE.
		TCL: Transaction Control Language(TCL) commands are used to manage transactions in the database. These are used to manage the changes made to the data in a table by DML statements. It also allows statements to be grouped together into logical transactions. BEGIN, SAVEPOINT, ROLLBACK, ABORT AND COMMIT .
	
	UNIQUE VS PRIMARY : 
		The main difference between the Primary key and Unique key is that the Primary key can never have a null value while the Unique key may consist of null value.
		In each table, there can be only one primary key while there can be more than one unique key in a table.
	
	Super Key: A superkey or super-key is defined in the relational model of database organization as a set of attributes of a relation variable for which it holds that in all relations assigned to that variable.

	Candidate Key : In the relational model of databases, a candidate key of a relation is a minimal superkey for that relation; that is, a set of attributes such that: the relation does not have two distinct tuples with the same values for these attributes there is no proper subset of these attributes for which holds.


	JOINS VS SUBQUERY: https://www.geeksforgeeks.org/sql-join-vs-subquery/

	Rows: record,tuple
	Column : attribute

	Truncate: Truncate operations drop and re-create the table without firing triggers(hence faster than delete)
	Delete: Removes elements in table given condition(optional)
	Drop : Deletes table & its schema
	Trim: Remove space char

	Joins:		Inner Join
				Left Join
				Right Join
				Full Join
				Self Join (Helps compare rows within the same table)
	Syntax 1 : SELECT T1.A,T1.B,T2.C FROM TABLE1 AS T1, TABLE2 AS T2 (X) JOIN ON T1.PRIMARY = T2.FOREIGN AND <condition 1> AND ... AND ...;
	Syntax 2 : SELECT T1.A,T1.B,T2.C FROM TABLE1 AS T1, TABLE2 AS T2 (X) JOIN ON T1.PRIMARY = T2.FOREIGN WHERE <condition 1> AND ... AND ...;
	Syntax 3 (explicit): SELECT T1.A,T1.B,T2.C FROM TABLE1 AS T1, TABLE2 AS T2 (X) WHERE T1.PRIMARY = T2.FOREIGN AND <condition 1> AND...;


	UNION VS UNION ALL: UNION removes duplicate , latter does not

	ACID:
		Atomicity: 
			This is based on the concept of “either all or nothing” which basically means that if any update occurs inside the database then that update should either be available to all the others beyond user and application program or it should not be available to anyone beyond the user and application program.
		Consistency: 
			This ensures that the consistency is maintained in the database before or after any transaction that takes place inside the database.
		Isolation: 
			As the name itself suggests, this property states that each transaction that occurs is in isolation with others i.e. a transaction which has started but not yet completed should be in isolation with others so that the other transaction does not get impacted with this transaction.
		Durability: 
			This property states that the data should always be in a durable state i.e. any data which is in the committed state should be available in the same state even if any failure or restart occurs in the system.

		MYSQL does not follow ACID , only transactions in MYSQL does. 

		CAP:	Consistancy: (All nodes see same data at same times)
				Availibility: Guarenteed response , success/failed
				Partition Tolerence: Can operate even if part of system fails.
				======> can only have 2 properties 

	Entity, Entity Type, Entity Set: 
		Entity is an object, place or thing which has its independent existence in the real world and about which data can be stored in a database. For Example, any person, book, etc. [ROWS]

		Entity Type is a collection of entities that have the same attributes. For Example, the STUDENT table contains rows in which each row is an entity holding the attributes like name, age, and id of the students, hence STUDENT is an Entity Type which holds the entities having the same attributes. [TABLES]

		Entity Set is a collection of entities of the same type. For Example, A collection of the employees of a firm. [SET OF ROWS/ENTITIES]

	Different levels of abstraction in DBMS:
		Physical Level: This is the lowest level of the data abstraction which states how the data is stored in the database.
		Logical Level: This is the next level of the data abstraction which states the type of the data and the relationship among the data that is stored in the database.
		View Level: This is the highest level in the data abstraction which shows/states only a part of the database.


	Normalization:
		1NF:	each attribute of a table must have atomic (single) values 
				each column must be of the same type
				each column must have different names
		2NF:	No partial Dependancies 
		3NF:	No Transitive Dependencies
		BCNF:	Every functional dependency X → Y, X is the super key of the table.
		4NF:	
		5NF:
		
Operating Systems:

Machine Learning:
	1. Project Workflow:
		Objective -> Define Problem -> Simple Baseline -> Research -> 	Setup Metrics -> EDA -> Partition Data -> Preprocess -> Model Creation -> Ensemble -> Deployment -> Monitor Model -> Iteration

	2. Cross Validation:
		Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a validation set to evaluate it. For example, a k-fold cross validation divides the data into k folds (or partitions), trains on each k-1 fold, and evaluate on the remaining 1 fold. This results to k models/evaluations, which can be averaged to get a overall model performance.
		I)	Exhaustive cross-validation : Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.

			i)	Leave-p-out cross-validation: Leave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set. LpO cross-validation requires training and validating the model nCp times.
			ii)	Leave-one-out cross-validation: More extreme example of leave p out cross validation , where p = 1.
		II)	Non-exhaustive cross-validation
			i)	Holdout method: Splitting of data into training and testing datsets and perform normal cross validation.
			ii) Repeated random sub-sampling validation: Repeated random subsampling validation also referred to as Monte Carlo cross-validation splits the dataset randomly into training and validation. Unlikely k-fold cross-validation split of the dataset into not in groups or folds but splits in this case in random.
			iii) K-fold cross-validation : A model is trained K number of times on different parts of the dataset as validation.
			iv) Stratified Varients : Class balanced cross validation

	3. Feature Importance:
		In linear models, feature importance can be calculated by the scale of the coefficients
		In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree. We can get a feature's importance for random forest by computing the averaging depth at which it appears across all trees in the forest.
		Methods to check Feature Importance:
			I) Pearson Correlation: Get absolute value of the Pearson’s Correlation between the target and numerical features in our dataset. Select top N as the features worth keeping.
			II) Chi-Squared: In this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.
			III) Recursive feature elimination (RFE): is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.
			IV) Select From Model : 
				i) Lasso Feature Selection
				ii) Tree Based

	4. Losses : 
		i) Rooted Mean Squared Error
		ii) Mean Absolute Error 
	5. L1 vs L2 regularization
	6. Correlation vs Covariance
	7. Would adding more data address underfitting
	8. Activation Function
	9. Bagging
	10. Stacking
	11. Generative vs discriminative
	12. Parametric vs Nonparametric
	13. Naive Baes
	14. SVM
	15. TF IDF , BM25 , RM3
	16. Decision Trees & Random Forest 
Computer Vision:

NLP:

Cloud Computing: 

	1) Services Provided: 
		> SaaS (Software as a service) [gmail,google drive]
		> PaaS (Platform as a service) : You can use a platform to create software applications. [google colab,	google app engine,magento cloud]
		> IaaS (Infrastructure as a service): Data , memory , cpu

		Cloud services handle virtualization(Virtually Dividing hardware, like CPU, RAM, GPU, OS, Storage, Server[Load Balancing], for multiple clients), servers, storage , networking.

	2) How can you use cloud computing for education / sales management? 
		Online Classes 
		Warehouses inventory in cloud from different shops

	3) Cloud Deployment Models:
		> Public Cloud: Can be accessed publically [WikiCloud/ Archive]
		> Private Cloud: Private Within a Team
		> Community Cloud: Companies collaborating with other companies
		> Hybrid Cloud: Combination of all  [Gdrive(some private some not)]

Big Data: 
	1) 3 Vs of Big data:
		> Velocity: Rate is large
		> Variety: Different kinds of data Generated
		> Volume: Volume increases overtime 
		> Value: Ability to turn data into value
		> Varacity: Refers to the biases, noise and abnormality in data. Is the data that is being stored, and mined meaningful to the problem being analyzed.
	2) Hadoop 
		> Open Source
		> Components:
			i) HDFS(Hadoop Distributed File System): 
				->  Uses YARN : Manages resources, shedules jobs . 
				->	Has Name Node(has jobtracker) which manages data nodes, which manges Data nodes. Data nodes(has tasktracker) perform computation and consists of data in duplication. (Master - Slave Connection).
			ii) Map Reduce
			iii) Hbase : DB for hadoop (Large Databases) 
			iv) Hive: SQL Language
			v)	Pig: Data flow platform (Executing map reducing writing smaller code)
			vi) Flume: Collects Logged Data
			vii) Spark : Built on top of hadoop, optimizes many things (map reduce speed, faster easer to work with)
			viii) Scoop : Helps transfer data from or to HBASE to other RDBMS

Internet Of Things:
	1) Any thing or device that can access network / internet and make decisions by itself.

Block Chain:
	1) To avoid single points of failures, especially for applications involving transactions, blockchain is used. How blockchain works:
		>	Instead of using one single server to manage a person's bank account , lets say, everyone has a copy of the information   (Called a Ledger). 
		>	If a transaction occurs then everyone is notified of this tranaction. If someone tampers with this data, and tries to do a transaction with it, it will find out by comparing the account details to others. 
		>	All data is encrypted. It is hashed, therefore on changing the hash value changes. The software sees if the current hash value is consistent with other copies then performs the transaction.
	2) Features of a Ledger: 
		>	Stores Encrypted Data
		>	Immutable Data
		>	Permanent Storage
		>	Chronological
		>	Secure
	3) Blocks are chained together in the form that each block will store some information about the previous blocks. If previous blocks are changed all other blocks will be changed , and hence no one can hack this distributed public ledger.

	4) Example : Bitcoin

	5) Cryptocurrencies:
	6) Bitcoin Mining: 
FULL STACK: 

	MVC: https://subscription.packtpub.com/book/web_development/9781787281080/1/ch01lvl1sec13/the-mvc-architectural-pattern



Common Interview Questions:
	1) Introduce Yourself: 
			Hello and good evening to all, my name is Arnab Bit, a final year student persuing computer engineering. I'm interested in Development and in the field of Deep Learning and have done multiple projects on it. I've also been working on and learning about deep learning for about one year focusing on fields like computer vision and natural language processing. The project that I'm most proud of is the also my final year project, which intends to explore novel attack strategies on android systems.  My Strength is Initiative to start the work immidiately , Good leadership skill for eg. i lead my team to qualification during the internal hackathon for the smart india hackathon, Adaptable to any kind of situation & Helping tendency. My Weakness is I am not comfortable, until I finish my work in the given time & over friendly in nature. My Short term goal is to get a development role in reputed company so to further develop my skillset. My Long term goal is to gain a postition of responsibility like a Tech-Lead and further mentor new talents in my company. Aside from the techinical aspects I enjoy swimming , running and learning about pop-science.

	2) Tell me about your projects: 
		i) APTIOS Eye Retinopathy: Diabetic Retinopathy is : Diabetic retinopathy is an eye condition that can cause vision loss and blindness in people who have diabetes. It affects blood vessels in the retina (the light-sensitive layer of tissue in the back of your eye).  
		In this competition we were given around 2K scanned images of eyes along with a severity level label thatwas provided by a clinician . Our job was to detect the severity of Retinopathy given any scanned image. We learnt many techniques for image pre-processing so that the image data was noise free( or at least contained
		the minimum presence of noise). We then used a efficient model (pretrained model ) and finetuned some of
		the layers to generate an output. After a while of hyperparameter training we are able to attain an accuracy
		of 75% without any sort of ensemble learning

		ii)FLASH IT : In this we developed a social media application for all age groups. It combines the features of already existing applications like snapchat , whatsapp etc along with few new ones which allow the user to combine the best elements of these applications. For instance, it keeps conversations more secure than Snapchat which despite its promise to notify the user when their chat is being screenshot-ed it is possible to bypass the feature. Flash it comes up with solutions for problems like these. It provides a secure and efficient way for users to share chats and media files. 
 



		iii) TV series Recommendation system: this was based on Content-Based Recommender Systems. A Content-based recommendation system tries to recommend items to users based on their profile. The user's profile revolves around that user's preferences and tastes. It is shaped based on user ratings, including the number of times that user has clicked on different items or perhaps even liked those items. 
		The recommendation process is based on the similarity between those items. 
		Similarity or closeness of items is measured based on the similarity in the content of those items. 
		When we say content, we're talking about things like the items category, tag, genre, and so on. 
		For example, if we have four movies, and if the user likes or rates the first two items, and if Item 3 is similar to Item 1 in terms of their genre, the engine will also recommend Item 3 to the user. we try to find a genre score of the user and compare it with the best rated show which is similar to the genre score
		The challenge faced is that this model won't recommend any show of a genre that the user haven't watched yet 


		


		iv) Room Classification : we were Given 10934 indoor-scenery images of Different Rooms to Train your model using computer vision techniques, transfer learning, data pre processing, and validation and testing, you have to classify the  test images on 67 different types of indoor-scenery images. This was also the Final Project for the Bits pilani QSTP Deep Learning course, which taught me a lot about Object Oriented Programming, use of functions, Testing , validation and debugging.
		

	3) Are you planning for masters ? 
		I don't have any plans for at least the next 4-5 years to do masters, I would like to explore the industry and other technolgies and fields. As great as research is, I can't imagine myself dedicating my life researching on one field for a masters then phd.

	4) Problems in group project?
		So I was a team leader for the hackathon we participated in, we only had 36 hours to make a working system. One of our group members were arguing with another about a certain algorithm. It would take a lot of time to write and train the algorithms so we could only choose one. To resolve this we looked at fourms and blogs and took a vote at the end.

	5)  Why do you want to work for this company?
		I see this opportunity as a way to contribute to an exciting and a fast-moving company, and I feel I can do so with my experience in various projects which taught me how to effectively Approach a problem and steps i needed to take while problem solving. Working with highly experienced individuals and industry professionals will fast track my progress,  seeing this i feel that its an invaluable opportunity to join such an esteemed company.
	





